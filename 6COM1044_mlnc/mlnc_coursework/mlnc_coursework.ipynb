{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6COM1044 - Data Classification Coursework\n",
    "- Marcelo Pedroza Hernandez\n",
    "- UH Student ID: 23033126\n",
    "- April 10, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used Libraries\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Data Exploration (20 marks)\n",
    "- In this task, you need to use Principal Component Analysis (PCA) to understand the characteristics of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 - (a)\n",
    "\n",
    "# Load the training and test datasets\n",
    "train_df = pd.read_csv(\"../mlnc_DATA/wdbc_training.csv\")\n",
    "test_df = pd.read_csv(\"../mlnc_DATA/wdbc_test.csv\")\n",
    "\n",
    "# Save the 30 features and the labels in separate variables for both datasets\n",
    "y_train = train_df.iloc[:, 1]\n",
    "X_train = train_df.iloc[:, 2:32]\n",
    "y_test = test_df.iloc[:, 1]\n",
    "X_test = test_df.iloc[:, 2:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 - (b)\n",
    "\n",
    "# Show a scatter plot of the first two features of the training dataset\n",
    "plt.scatter(X_train.iloc[:, 0], X_train.iloc[:, 1], c=y_train)\n",
    "plt.xlabel('Feature 1', fontweight='bold')\n",
    "plt.ylabel('Feature 2', fontweight='bold')\n",
    "plt.title('Scatter plot of the first two features of the training dataset', fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 - (c)\n",
    "\n",
    "# Normalise the training and test datasets using StandardScaler() from sklearn\n",
    "X_train_norm = StandardScaler().fit_transform(X_train)\n",
    "X_test_norm = StandardScaler().fit_transform(X_test)\n",
    "\n",
    "# Report the mean and standard deviation for the first feature in the normalized test set\n",
    "mean1 = np.mean(X_test_norm[:, 0])\n",
    "print(mean1)  # Near 0 = OK\n",
    "std1 = np.std(X_test_norm[:, 0])\n",
    "print(std1)  # Near 1 (unit std) = OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 - (d)\n",
    "\n",
    "# Perform a PCA analysis on the scaled training set\n",
    "pca = PCA()\n",
    "projections = pca.fit_transform(X_train_norm)  # The projections matrix represents the eigenvectors of the covariance matrix\n",
    "print(f\"PCA Projections Shape: {projections.shape}\")  # Confirm the shape of the projections matrix matches the number of features\n",
    "\n",
    "# Report how much variance has been captured in the PCA analysis\n",
    "variance_ratios = pca.explained_variance_ratio_\n",
    "print(f\"\\nVariance Captured by each PCA component:\\n {variance_ratios}\")  # Order of variance ratios is the order of the PCA components\n",
    "variance_ratio_sum = np.sum(variance_ratios)  # Confirm the sum of the variance ratios is 1\n",
    "print(f\"\\nSum of Variance Captured by PCA components: {variance_ratio_sum}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 - (d) Plots\n",
    "\n",
    "# Two subplots in one figure\n",
    "figure1, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Projection of the training set in PC1 and PC2\n",
    "ax[0].scatter(projections[:, 0], projections[:, 1], c=y_train) \n",
    "ax[0].set_xlabel('PC1', fontweight='bold')\n",
    "ax[0].set_ylabel('PC2', fontweight='bold')\n",
    "ax[0].set_title('Projection of the training set in the first two principal components', fontweight='bold', fontsize=11)\n",
    "\n",
    "# Scree plot of the variance captured by each component\n",
    "ax[1].plot(range(1, 31), variance_ratios, marker='.', color='orange') \n",
    "ax[1].set_xlabel('Principal Components', fontweight='bold')\n",
    "ax[1].set_ylabel('Variance Captured', fontweight='bold')\n",
    "ax[1].set_title('Scree Plot', fontweight='bold')\n",
    " \n",
    "plt.show()\n",
    "figure1.savefig('Task1_D_Plots.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Data Preparation (5 marks)\n",
    "- In this task, you need to divide the training dataset into a smaller training set and a validation set, and normalise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 - (a)\n",
    "\n",
    "# Divide the training set using 30% as the validation set, randomly selecting the points\n",
    "X_train_II, X_val, y_train_II, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=38)\n",
    "# X_train_II, X_val, y_train_II, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=38, stratify=y_train)  # Stratify the split to maintain the class distribution\n",
    "\n",
    "# Report the number of points in each set\n",
    "print(f\"Number of points in the smaller training set: {X_train_II.shape[0]}\")\n",
    "print(f\"Number of points in the validation set: {X_val.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 - (b)\n",
    "\n",
    "# Normalise the smaller training set (II)\n",
    "X_train_II_norm = StandardScaler().fit_transform(X_train_II)\n",
    "mean2 = np.mean(X_train_II_norm[:, 0])\n",
    "print(mean2)  # Near 0 = OK\n",
    "std2 = np.std(X_train_II_norm[:, 0])\n",
    "print(std2)  # Near 1 (unit std) = OK\n",
    "\n",
    "# Normalise the validation set\n",
    "X_val_norm = StandardScaler().fit_transform(X_val)\n",
    "mean3 = np.mean(X_val_norm[:, 0])\n",
    "std3 = np.std(X_val_norm[:, 0])\n",
    "print(std3)  # Near 1 (unit std) = OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - SVM Classification (12 marks)\n",
    "- In this task, you need to build a support vector classifier using SVC from sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3 - (a) Linear Kernel\n",
    "\n",
    "# Evaluate performance using a linear kernel with three different C values \n",
    "print(\"\\nClassification Reports for Linear Kernel with Different C Values:\")\n",
    "svc1_lin = SVC(kernel='linear', C=2)  # Smallest C value\n",
    "model1_lin = svc1_lin.fit(X_train_II_norm, y_train_II)\n",
    "y_pred1_lin = model1_lin.predict(X_val_norm)\n",
    "print(f\"Classification Report for C=2:\\n{classification_report(y_val, y_pred1_lin)}\")\n",
    "\n",
    "svc2_lin = SVC(kernel='linear', C=27)  # Middle C value\n",
    "model2_lin = svc2_lin.fit(X_train_II_norm, y_train_II)\n",
    "y_pred2_lin = model2_lin.predict(X_val_norm)\n",
    "print(f\"Classification Report for C=27:\\n{classification_report(y_val, y_pred2_lin)}\")\n",
    "\n",
    "svc3_lin = SVC(kernel='linear', C=52)  # Largest C value\n",
    "model3_lin = svc3_lin.fit(X_train_II_norm, y_train_II)\n",
    "y_pred3_lin = model3_lin.predict(X_val_norm)\n",
    "print(f\"Classification Report for C=52:\\n{classification_report(y_val, y_pred3_lin)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3 - (a) RBF Kernel\n",
    "# Define parameter grid for the GridSearchCV for finding the best C and gamma values\n",
    "# param_grid = {'C': np.arange(2, 52, 1), 'gamma': np.arange(0.01, 12, 0.025)} \n",
    "\n",
    "# svm_rbf = SVC(kernel='rbf')\n",
    "\n",
    "# # Perform a grid search to find the best C and gamma values\n",
    "# grid_search = GridSearchCV(svm_rbf, param_grid=param_grid, cv=20, scoring='accuracy', n_jobs=-1)\n",
    "# grid_search.fit(X_train_II_norm, y_train_II)\n",
    "\n",
    "# # Find the best parameter combination\n",
    "# best_params = grid_search.best_params_\n",
    "# best_score = grid_search.best_score_\n",
    "\n",
    "# print(f\"Best Parameters: {best_params}\")\n",
    "# print(f\"Best Mean Cross-Validated Score (accuracy): {best_score:.3f}\")\n",
    "\n",
    "# # Extract and report the top three combinations of C and gamma values based on rank\n",
    "# results = grid_search.cv_results_\n",
    "# sorted_indices = np.argsort(results['mean_test_score'])[::-1]  # Sorting in descending order\n",
    "\n",
    "# print(\"\\nTop Three Combinations of C and gamma Values with Classification Reports:\")\n",
    "\n",
    "# for rank, index in enumerate(sorted_indices[:3], start=1):  # Just the top 3\n",
    "#     best_C = results['param_C'][index]\n",
    "#     best_gamma = results['param_gamma'][index]\n",
    "    \n",
    "#     # Since GridSearchCV refits the best model, only detail the top combination here\n",
    "#     if rank == 1:\n",
    "#         best_model = grid_search.best_estimator_  # The model refitted with the best parameters\n",
    "#         y_pred = best_model.predict(X_val_norm)\n",
    "#     else:\n",
    "#         # Manually train models for the 2nd and 3rd best parameter sets\n",
    "#         best_model = SVC(kernel='rbf', C=best_C, gamma=best_gamma)\n",
    "#         best_model.fit(X_train_II_norm, y_train_II)\n",
    "#         y_pred = best_model.predict(X_val_norm)\n",
    "    \n",
    "#     accuracy = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "#     print(f\"\\nRank: {rank}\")\n",
    "#     print(f\"Parameters - C: {best_C}, gamma: {best_gamma}\")\n",
    "#     print(f\"Accuracy: {accuracy:.3f}\")\n",
    "#     print(\"Classification Report:\")\n",
    "#     print(classification_report(y_val, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################\n",
    "# Evaluate performance using an RBF kernel with the top three best combinations of C and gamma values found from the randomised search\n",
    "\n",
    "# Define parameter grid for the randomised cross-validation search for the best C and gamma values\n",
    "param_grid = {'C': np.arange(2, 52, 1), 'gamma': np.arange(0.01, 12, 0.01)} \n",
    "\n",
    "svm_rbf = SVC(kernel='rbf')\n",
    "\n",
    "# Perform a randomised search for the best C and gamma values\n",
    "random_search = RandomizedSearchCV(svm_rbf, param_distributions=param_grid, n_iter=200, cv=100, random_state=38, n_jobs=-1)\n",
    "random_search.fit(X_train_II_norm, y_train_II)\n",
    "\n",
    "# Extract the results\n",
    "results = random_search.cv_results_\n",
    "\n",
    "# Sort the scores and get indices of top three combinations\n",
    "top_three_indices = np.argsort(results['mean_test_score'])[-3:]\n",
    "\n",
    "# Report the top three combinations of C and gamma values\n",
    "print(\"\\nTop Three Combinations of C and gamma Values:\")\n",
    "for rank, index in enumerate(top_three_indices, start=1):\n",
    "    print(f\"{rank}. Mean validation score: {results['mean_test_score'][index]:.3f} (std: {results['std_test_score'][index]:.3f})\")\n",
    "    print(f\"   C: {results['param_C'][index]}, gamma: {results['param_gamma'][index]}\")\n",
    "\n",
    "######################################################################################################\n",
    "# Define parameter grid for the cross-validation search for the best C and gamma values\n",
    "# svm1_rbf = SVC(kernel='rbf', C=2, gamma=0.01)  # Smallest C and gamma values\n",
    "# model1_rbf = svm1_rbf.fit(X_train_II_norm, y_train_II)\n",
    "# y_pred1_rbf = model1_rbf.predict(X_val_norm)\n",
    "# print(f\"Classification Report for C=2 and gamma=0.01:\\n{classification_report(y_val, y_pred1_rbf)}\")\n",
    "\n",
    "# svm2_rbf = SVC(kernel='rbf', C=27, gamma=0.35)  # Middle C and geometric mean of gamma values\n",
    "# model2_rbf = svm2_rbf.fit(X_train_II_norm, y_train_II)\n",
    "# y_pred2_rbf = model2_rbf.predict(X_val_norm)\n",
    "# print(f\"Classification Report for C=27 and gamma=0.35:\\n{classification_report(y_val, y_pred2_rbf)}\")\n",
    "\n",
    "# svm3_rbf = SVC(kernel='rbf', C=52, gamma=12)  # Largest C and gamma values\n",
    "# model3_rbf = svm3_rbf.fit(X_train_II_norm, y_train_II)\n",
    "# y_pred3_rbf = model3_rbf.predict(X_val_norm)\n",
    "# print(f\"Classification Report for C=52 and gamma=12:\\n{classification_report(y_val, y_pred3_rbf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"train an SVM model with the suitable parameter values discovered\n",
    "in Task 3 (b) on the normalised whole training set (I). Report the classification report and confusion matrix\"\"\"\n",
    "\n",
    "# Task 3 - (c) Further Evaluation of the Best-Performing Model (RBF Kernel with C=2 and gamma=0.02)\n",
    "\n",
    "# Train the best-performing model on the normalised whole training set (I)\n",
    "# svm_best = SVC(kernel='rbf', C=27, gamma=0.2) # Best C and gamma values from Task 3 (a)\n",
    "svm_best = SVC(kernel='rbf', C=20, gamma=0.2)\n",
    "model_best = svm_best.fit(X_train_norm, y_train)\n",
    "y_pred_best = model_best.predict(X_train_norm)\n",
    "print(f\"Classification Report for the best-performing model:\\n{classification_report(y_train, y_pred_best)}\")\n",
    "\n",
    "model_test = svm_best.fit(X_test_norm, y_test)\n",
    "y_pred_test = model_test.predict(X_test_norm)\n",
    "print(f\"Classification Report for the best-performing model on the test set:\\n{classification_report(y_test, y_pred_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
