{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Associative Neural Networks (ANNs)\n",
    "\n",
    "### Associative Neural Networks (ANN) are a type of artificial neural network that can learn associations between input patterns and corresponding output patterns. \n",
    "### They are commonly used for tasks such as pattern recognition, classification, and prediction.\n",
    "\n",
    "#### How ANN Works:\n",
    "    1. Data Preparation: Prepare the input-output pairs for training the ANN. Each input pattern is associated with a corresponding output pattern.\n",
    "\n",
    "    2. Network Architecture: Define the architecture of the ANN, including the number of input and output neurons, as well as the number and configuration of hidden layers.\n",
    "\n",
    "    3. Weight Initialization: Initialize the weights of the connections between neurons randomly or using specific techniques like Xavier or He initialization.\n",
    "\n",
    "    4. Forward Propagation: Pass the input pattern through the network to compute the output. This involves multiplying the input values with the corresponding weights, applying an activation function, and passing the result to the next layer.\n",
    "\n",
    "    5. Error Calculation: Compare the computed output with the expected output and calculate the error using a suitable loss function, such as mean squared error (MSE) or cross-entropy loss.\n",
    "\n",
    "    6. Backpropagation: Update the weights of the connections in the network to minimize the error. This is done by propagating the error backward through the network, adjusting the weights based on the error gradient and the learning rate.\n",
    "\n",
    "    7. Training: Repeat steps 4-6 for multiple iterations or epochs until the network learns the associations between input and output patterns.\n",
    "\n",
    "    8. Testing: Evaluate the performance of the trained ANN on unseen data to assess its accuracy and generalization capabilities.\n",
    "\n",
    "    ***Note that a tensor is a multi-dimensional array similar to NumPy arrays, but with additional features such as GPU acceleration and automatic differentiation for gradient-based optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ANN architecture\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ANN, self).__init__()  # Call the constructor of the parent class (nn.Module)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Fully connected layer\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)  # Fully connected layer\n",
    "        self.activation = nn.ReLU()  # Activation function\n",
    "\n",
    "    # Define the forward pass\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)  # Forward pass through the first fully connected layer\n",
    "        x = self.activation(x)  # Apply the activation function\n",
    "        x = self.fc2(x)  # Forward pass through the second fully connected layer\n",
    "        return x  # Return the output   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 1.]])\n",
      "\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "# Prepare the input-output pairs\n",
    "input_data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)  # Input patterns in a tensor format (4x2)\n",
    "output_data = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)  # Output patterns in a tensor format (4x1)\n",
    "\n",
    "print(input_data)\n",
    "print()\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN(\n",
      "  (fc1): Linear(in_features=2, out_features=4, bias=True)\n",
      "  (fc2): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (activation): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ANN\n",
    "input_size = 2  # Number of input neurons\n",
    "hidden_size = 4  # Number of neurons in the hidden layer\n",
    "output_size = 1  # Number of output neurons\n",
    "ann = ANN(input_size, hidden_size, output_size)  # Create an instance of the ANN\n",
    "\n",
    "print(ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSELoss()\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.1\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function and optimizer\n",
    "# Mean squared error loss (standard loss function for regression tasks)\n",
    "criterion = nn.MSELoss()  \n",
    "# Stochastic gradient descent optimizer with learning rate 0.1 (standard optimizer for training neural networks)\n",
    "optimizer = optim.SGD(ann.parameters(), lr=0.1)  \n",
    "\n",
    "print(criterion)\n",
    "print()\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.1320\n",
      "Epoch [200/1000], Loss: 0.0292\n",
      "Epoch [300/1000], Loss: 0.0007\n",
      "Epoch [400/1000], Loss: 0.0000\n",
      "Epoch [500/1000], Loss: 0.0000\n",
      "Epoch [600/1000], Loss: 0.0000\n",
      "Epoch [700/1000], Loss: 0.0000\n",
      "Epoch [800/1000], Loss: 0.0000\n",
      "Epoch [900/1000], Loss: 0.0000\n",
      "Epoch [1000/1000], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the ANN\n",
    "num_epochs = 1000  # Number of training epochs (iterations)\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Clear the gradients\n",
    "    output = ann(input_data)  # Forward pass\n",
    "    loss = criterion(output, output_data)  # Compute the loss\n",
    "    loss.backward()  # Backpropagation to compute the gradients\n",
    "    optimizer.step()  # Update the weights based on the gradients\n",
    "    \n",
    "    # Print the loss at every 100 epochs for monitoring\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.7684e-07],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [5.9605e-07]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test the trained ANN\n",
    "test_input = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)  # Test input patterns\n",
    "test_output = ann(test_input)  # Predict the output\n",
    "print(test_output)  # Print the predicted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F1 Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "# The performance of the trained ANN can be evaluated using metrics such as accuracy, precision, recall, and F1 score\n",
    " \n",
    "# Accuracy:\n",
    "correct_predictions = (test_output.round() == output_data).sum().item()\n",
    "total_predictions = output_data.size(0)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Precision:\n",
    "true_positives = ((test_output.round() == 1) & (output_data == 1)).sum().item()\n",
    "false_positives = ((test_output.round() == 1) & (output_data == 0)).sum().item()\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "print(f'Precision: {precision:.2f}')\n",
    "\n",
    "# Recall:\n",
    "false_negatives = ((test_output.round() == 0) & (output_data == 1)).sum().item()\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "print(f'Recall: {recall:.2f}')\n",
    "\n",
    "# F1 score:\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "print(f'F1 Score: {f1_score:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
